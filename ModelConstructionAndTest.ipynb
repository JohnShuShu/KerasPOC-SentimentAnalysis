{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exacon02/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "from tensorflow import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif,chi2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Dataset from http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exacon02/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "training = np.genfromtxt('Sentiment Analysis Dataset.csv', delimiter=',', skip_header=1, usecols=(1, 3), dtype=None)\n",
    "\n",
    "train_x = [x[1] for x in training]\n",
    "train_y = np.asarray([x[0] for x in training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "kwargs = {\n",
    "            'ngram_range': (1,2),  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "            'min_df':  2,\n",
    "}\n",
    "vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "train_x = vectorizer.fit_transform(train_x)\n",
    "\n",
    "# Select top 'k' of the vectorized features.\n",
    "selector = SelectKBest(chi2, k=min(20000, train_x.shape[1]))\n",
    "train_x = selector.fit_transform(train_x, train_y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1262901 samples, validate on 315726 samples\n",
      "Epoch 1/20\n",
      "1262901/1262901 [==============================] - 298s 236us/step - loss: 0.5203 - acc: 0.7460 - val_loss: 0.4470 - val_acc: 0.7952\n",
      "Epoch 2/20\n",
      "1262901/1262901 [==============================] - 297s 236us/step - loss: 0.4903 - acc: 0.7619 - val_loss: 0.4400 - val_acc: 0.7976\n",
      "Epoch 3/20\n",
      "1262901/1262901 [==============================] - 299s 237us/step - loss: 0.4864 - acc: 0.7634 - val_loss: 0.4376 - val_acc: 0.7990\n",
      "Epoch 4/20\n",
      "1262901/1262901 [==============================] - 295s 234us/step - loss: 0.4849 - acc: 0.7642 - val_loss: 0.4378 - val_acc: 0.7987\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameters for the model\n",
    "layers=2\n",
    "units=32\n",
    "dropout_rate=0.3\n",
    "input_shape=train_x.shape[1:]\n",
    "num_classes=2\n",
    "\n",
    "\n",
    "# Create model instance.\n",
    "model=mlp_model(layers,units,dropout_rate,input_shape,num_classes)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Train and validate model.\n",
    "model.fit(train_x, train_y,\n",
    "  batch_size=512,\n",
    "  epochs=20,\n",
    "  verbose=1,\n",
    "  validation_split=0.2,\n",
    "  callbacks = [EarlyStopping(monitor='val_loss', patience=1)],     \n",
    "  shuffle=True)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a sentence to be evaluated, or Enter to quit: i am very sad\n",
      "Negative Sentiment.Confidence Level: 99.68916536308825 %\n",
      "Input a sentence to be evaluated, or Enter to quit: he is a coward\n",
      "Positive Sentiment.Confidence Level: 30.955064296722412 %\n",
      "Input a sentence to be evaluated, or Enter to quit: he is a very genuine person\n",
      "Positive Sentiment.Confidence Level: 30.14744520187378 %\n",
      "Input a sentence to be evaluated, or Enter to quit: she is a disgrace to the famil\n",
      "Positive Sentiment.Confidence Level: 19.513344764709473 %\n",
      "Input a sentence to be evaluated, or Enter to quit: he is a disgrace\n",
      "Positive Sentiment.Confidence Level: 29.87825870513916 %\n",
      "Input a sentence to be evaluated, or Enter to quit: she is a disgrace to the family\n",
      "Positive Sentiment.Confidence Level: 27.11465358734131 %\n",
      "Input a sentence to be evaluated, or Enter to quit: A miracle happened that night\n",
      "Negative Sentiment.Confidence Level: 39.957815408706665 %\n",
      "Input a sentence to be evaluated, or Enter to quit: fuck off\n",
      "Negative Sentiment.Confidence Level: 44.412070512771606 %\n",
      "Input a sentence to be evaluated, or Enter to quit: she is amazing\n",
      "Positive Sentiment.Confidence Level: 78.82722616195679 %\n",
      "Input a sentence to be evaluated, or Enter to quit: not bad\n",
      "Positive Sentiment.Confidence Level: 20.124053955078125 %\n",
      "Input a sentence to be evaluated, or Enter to quit: this is not bad\n",
      "Positive Sentiment.Confidence Level: 2.603161334991455 %\n",
      "Input a sentence to be evaluated, or Enter to quit: this is not good\n",
      "Negative Sentiment.Confidence Level: 95.575001090765 %\n",
      "Input a sentence to be evaluated, or Enter to quit: \n"
     ]
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')\n",
    "while 1:\n",
    "    evalSentence =input('Input a sentence to be evaluated, or Enter to quit: ')\n",
    "\n",
    "    if len(evalSentence) == 0:\n",
    "        break\n",
    "\n",
    "    # Format your input for the neural net\n",
    "    evalSentence=[evalSentence]\n",
    "    tx = vectorizer.transform(evalSentence).astype('float32')\n",
    "    tx = selector.transform(tx).astype('float32')\n",
    "    pred = model.predict(tx)\n",
    "    if pred[0][0]>0.5:\n",
    "        print(\"Positive Sentiment.Confidence Level:\",(pred[0][0]-0.5)*200,\"%\")\n",
    "    else:\n",
    "        print(\"Negative Sentiment.Confidence Level:\",(0.5-pred[0][0])*200,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
